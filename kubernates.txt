

[Kubernates made easy tutorial](https://www.youtube.com/watch?v=6uvHVVNq34w&list=PLMPZQTftRCS8Pp4wiiUruly5ODScvAwcQ&index=18)

** Kubernates Architecture**

Kubernates master
 1)managing each operatiion and node. 
 2) checking each node is healthy , pod is healthy ,scheduling deployemnet of pod , controlling replicas.
  
 
**Master components**
 
    -API server : API is used to make communication between two pod or two service in kubernates. to manage all this API's we 
     have API server . it will manage communication between different component of KUBERNATES.

     it isimp component in Kubernates cluster. it is front end component of Kubernates master. we define our cluster in yaml file.
                    API server will validate that file and deploy continer to cluster.

    -etcd : It is a place where you store your password. it is store data in form of key value pair. 
 
    -Kube controller mnager :  handle all the controller that is present in kubernates.
      node controller : it will check wheather a node is created , in progress or ready state.
      replica controller : it will ensure that actual state is equal to desired state.

     it is having many controller in cluster which will check wheather desired state of cluster is 
     equal to actual state of cluster.if desired state is not equal to actual state . controller manager will run watch loop at 
	 backend and bring desired stte equl to actual state.
	  
	  
    -kube scheduler  : it will distribute the work load to nodes in kubernates cluster. example which job needs to run on which node.


    -clould controller manager : making kubenates cluster avaiable to hardware of the cloud. it act as bridge between the 
     aws hardware ( sevices how aws works ) and kubernates API ( how thwy will work).
 

 
 -Kubernates Nodes 
    
	- Kubelet : it is major compont .it is kubernates agent on the node.it will interct with api server and deploy the contaner.
	  it will take work load from kubernates master
	
	-Kubeproxy : it will see the newtwork aspect betwen user and container.it will check netwrk 
	  connection betwee user and contaienr how they are expose to external world.
	
	-cAdvisor : container run time environment. which container we are using example DOcker , Rocket.
	
	- pod: very imp. continer will run inside pod. it will create runtime env for container. provide layer of abstarction.
	       as standard we will run 1 container per pod. we can run many.
    
	-kubectl : its a way of acesing the cluster. it is a command line tool.


Kubectl syntaz

kubectl [command] [resource type] [resource name] [flag]
==============================================================================================================================
Flags
-f : file
-o : output
-l : label
-c : container name
-record : recird the command in history
--replicas=5

kubectl get pods --selector=app=ngnx-app

kubectl cluster-info
---------------------------------------------------------
kubectl create -f pod.yml 
Kubectl create -f < directoty name>
--------------------------------------------------------
Kubectl get [resource type] [Name] [flag]

kubectl get pods 
this will display high level infomation about the pods
--------------------------------------------------------
Kubectl describe [type] [name] -- display complete information  1 or more resources.

kubectl describe pods

kubectl describe pods <pod-name>
kubectl describe node <node-name>
---------------------------------------------------------------
kubectl delete pods <pod name>

kubectl delete pods,servcie -l name=<labelname>

-l flag is for Label

kubectl delete pods -all 

it will delete all the pods
---------------------------------------------------------------
Kubectl exec --> execute a command against a container inside a pod

kubectl exec <pod-name> date   ( when there is one container inside a pod)
kubectl exec <pod-name> -c <conatainer name > date  ( when there is more than one conatainer in pod)

kubectl exec <pod-name> -it <pod-name> /bin/bash  ( to get inside container)

------------------------------------------

kubectl log pod-name

kubectl log -f pod-name

------------------------------------------
PODS:-
Continer inside pod share same IP address ,same volumes 

all pods can communicate with each other

Pod life cycle:-

we write the mainfeast file and submit it to API server.
it will create container using Mainfest file.
it will be in pending state until all conatiners is created.
once all conatiners are up and running it will go to running state.
once its work its completed it go to successded state.
if any failure happens it will go to failed state.

Not ready and Unkown state.
-----------------------------------------------
Pod Config 

#nginix-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx-label
    tier: dev
spec:
  containers:
    - name: nginx-conatiner
      image: nginx
    
    
Command to excuete (youtube vdeio 21:00)

1) kubectl create -f niginx-pod.ymal
2) kubectl get pods
3) kubectl get pod -o wide ( show wide output or more option like Ipaddress , nOde)
4) kubectl get pod nginix-pod -o yaml  ( it will give detail in yaml format)
kubectl get pod nginix-pod -o yaml | more
  
5) kubectl decribe pod nginix-pod ( detailed output of nginx pod
6) kubectl exec -it pod nginix-pod -- /bin/bash ( get inside continer)

run some commnds
hostname
exit
7) kubectl expose pod nginix-pod --type=NodePort --port:80   ( Port forwarding)
   
8)kubectl delete pod nginix-pod
------------------------------------------

Replication controller 

It ensure specefic number ( mentioned in yaml file) of pods are always running. If there excess pods it gets killed and vice versa.

Replication COntroller and Pods are assosciated with Labels

Advantages :
 High availability
 Load balaancing
 
 
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
 Mainfest file:-
 #niginx-rc.yaml
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-contr

spec:
  replicas: 3
  selector:
    app: nginx-app
  template:
    metadata:
      name: nginx-pod
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx-cont
        image: nginx
        ports:
        - containerPort: 80
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Command to excuete 	   
1) kubectl create -f niginx-rc.ymal
2) kubectl get pods
3) kubectl get pod -l app:nginx-app  (search with label name)
 
4) kubectl decribe rc nginix-rc 
5) kubectl get pod -o wide

we can check one pod is running on node 2. we will manually turnoff the node 2 and check the status

pod will go to not ready state.


6) kubectl get nodes

after some time again check the status wuth wide output option
we will see that status of pod on node 2 become unkown ann new  pod is runnuing on Node 1

Scale up and down

7 kubectl scale  rc niginix-rd --replicas=5
8 kubectl scale  rc niginix-rd --replicas=2

kubectl get rc niginx-rd

kubectl get pods

DELETE

kubectl delete -f nigin.rc.yaml

kubectl get rc

kubectl get pods

-------------------------------------------------------------------------------------
ReplicaSet

Difference between RelicaSet and ReplicationController is that replicaset uses Set based selector where as
replication controller select equity based selector.
------------------------------------------
Equality based Selctor have 3 operator:-  = == !=   ( equal. double equal , Not equal)
example:-
 environemnt=production
 tier!=frontend 

 Command line
 
 kubectl get pods -l environemnt=production
 
 Mianfiest file
 
 selector:
    environemnt=production
    tier!=frontend 
------------------------------------------
Set Based Selector have 3 operator:- in notin  exist 
Example
 environemnt in (production,qa)
 tier notin (frontend,backend)
 
 in set based we can select more than one value.
 
we selcting pods from set of actions and that is why it is called as set based selector.

Supports:
services and replication comtroller

COmmand line 

kubectl get pods -l environment in ( production ) 

Mainfeast file:

selector:
  matchExpression:
  - {key:environment,operator:in,value:[production]}
  - {key:tier,operator:notin, value:[frontend,backend]}
  or 
  matchLabels:
    app:environment
  

supports:
 replica set, job , deployement,damon set.
 
 -----------------------------------------------------
 
 Two different types of selector:
 
 selector :                        selector:
   app:Enviroment                    matchLabel:
                                       app:Environment
Old resources
Replication Controller	       New Resources like RelicaSet, Jobs,Dameon set
services
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#nginx-rs.yaml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replica-set
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      name: nginx-pod
      labels:
        app: nginx-app
        tier: dev
    spec:
     containers:
     - name: nginx-container
       image: nginx
       ports:
       - containerPort: 80




xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

commands to execute:

1) kubectl create -f nigini-rs.yaml
2) kubectl get pods ( 3instances of pod is created)
3)kubectl get pods -l tier ( serach pods with label if there are many pods runnung)
4)kubectl get rs nginx-rs -o wide ( at wide optput you can see both selector is displaying)
5)kubectl describe rs nginx-rs

kubectl scheduling , stop node 2 and check kubectl gets pod -o wide command to see pod gets crested in another node.

6) kubectl scale rs nginx-rs --replicas=5
7) kubectl scale rs nginx-rs --replicas=1
8) kubectl scale rs nginx-rs ( check desired and currect is same)
9) kubectl delete -f nginx-rs.yaml
10)kubectl get pods -l app:nginx-app
-------------------------------------------------------------------------------------
Deployement 

deployemnts is all about updates and rollbacks.

Features 

Multiple replicas
upgrade
rollback
scale up or down
pause and resume

https://www.youtube.com/watch?v=mNK14yXIZF4&list=PLLasX02E8BPCrIhFrc_ZiINhbRkYMKdPT&index=5

Liveness check ;-it define pod should automaticaly restarted
readyness check;- it define wheather it is ready to serve
termination grace period :- default 30 sec . it define how long container should be up and running after its been requested 
for deletion.to actual deletion.



Deployement types

1) Recreate :- its is a dummy deployemnt process. in this we will first shutdown version A and once version A is shutdown,
then we will deploy version B. while switching from versio A to version B there will be a down time of server

2) Rolling update:- in this , it will slowly update the version of the app by replacing instances one after the other until
all instances are succesfully rolled out
 default stragey , easy to use but take some time.

3)Canary deployemnt:- it is gradually (slowly) shifting form vesion A to Verion B. first will upgrade 2 instance out of 10 to verB
and will do some testing if every thing is good we will upgrade 8 instances. and shutdown versionA.
this is goood for testing.

4) Blue/green deployemnt:-  here blue means version A and Green means Verison B. we will create exact same no of version B instance
and test the instance , once it is tested we will change the traffic to Version B at load balancer level.

advatage of this is instant update and rollback. 

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Deployment mainfest file.
#niginix-deploy.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-nginx-prod
  labels:
    deployapp: dev
spec:
  replicas: 3
  selector:
    matchLabels:
      podapp: dev


  template:
    metadata:
      name: nginx-pod-deploy
      labels:
        podapp: dev
        podtier: prod
    spec:
      containers:
      - name: nginx-container-deploy
        image: nginx:1.7.9
        ports:
        - containerPort: 80

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Commands:
1 kubectl create -f niginix-deploy.yaml

2) kubectl get deploy -l app=niginix-app

3) kubectl get rs -l app=niginix-app  ( dispaly replica set)

4) kubectl get pods -l app=niginix-app

5) kubectl describe deploy nginix-deployemnt

------update Deployemnt----------------

there are two ways to update it 
1)  SET command

kubectl set image deploy nginx deployement ngnix-conatiner=nginix:1.9.1

2)  edit command
kubectl edit deploy nginx-deployement

in this configuration will open in Vi editor , make the changes and close it, configuraton will save automatically


check the status using 

kuebctl rollout status deployemnt/nginx-deployment

kubectl get deploy 


------ RollBack Deployemnt-------------------------

suppose if we gave the wrong version number as 1.9.1 as 1.91 then 

1) kubectl set image deploy nginx deployement ngnix-conatiner=nginix:1.91 --record

record will record this command in history

once the command is run output shows that image is updated.

2)kubectl rollout status deployemnt/nginx-deployment

but when you check the statuus it will show that it is stuck or waiting for somethibg

3) kubectl rollout history deployemnt/nginx-deployemnt

to check which commandwe have executed earlier

we will notice that incorrect version number. to undo this deployment will execute below command

4) kubectl rollout undo deployment/nginix-deployment

now again you can check the status
5)kubectl rollout status deployemnt/nginx-deployment


Scale Up and Scale Down 

1) kubectl scale deployment nginix-deployment --replicas=5

it will scale deployment form 3 to 5

2) kubectl get deploy
3) kubectl het pods


4)  kubectl scale deployment nginix-deployment --replicas=2

it will scale deployment form 5 to 2


Delete

Kubectl delete -g nginx-deploy.yaml












==================================================
Horizontal scaling means that you scale by adding more machines
into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM) to an existing machine.


====================================================================================================================================

Servicess


why we need services

Ip adress changes when pod dies
communication between pods
aplicatioon expose to external world


Services is way of grouping of pods that are running on cluster.

features
- zero downtime app deployemnt
- service dicovery between apps
- load balancing


#3 Types of Sevices

1) Node Port service  :- used for port forwarding
2) Load balancing service
3) Cluster service :-it is reachable only inside the cluster, to connect frontend and backend pods. 




Nord Pord Service:-

node port is basically port forwarding of Pod application to node Ip and port.

Node port range- 31000 to 32767

it is 3 ipaddress and Port

1) node ip and Node port
2) service ip and service port ( Node port service IP)
3) target ip and target port ( Pod IP)

Node port service can be access accross Nodes.

Downside

one service per Port 
if vm ip changes u need to deal with that
 
++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++
*******************************************************************

1. Deployment & NodePort service manifest file

Deployment YAML file:
~~~~~~~~~~~~~~~~~~~~~
# Deployment
# nginx-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.7.9
        ports:
        - containerPort: 80

--------------------------------------

NodePort Service YAML file:
~~~~~~~~~~~~~~~~~~~~~~~~~~
# Service
# nginx-svc-np.yaml
apiVersion: v1
kind: Service	
metadata:
  name: my-service
  labels:
    app: nginx-app
spec:
  selector:
    app: nginx-app
  type: NodePort
  ports:
  - nodePort: 31111
    port: 80
    targetPort: 80

*******************************************************************
2. Create and Display Deployment and NodePort

kubectl create –f nginx-deploy.yaml
kubectl create -f nginx-svc.yaml
kubectl get service -l app=nginx-app
kubectl get po -o wide
kubectl describe svc my-service

*******************************************************************
3. Testing

# To get inside the pod
kubectl exec [POD-IP] -it /bin/sh

# Create test HTML page
cat <<EOF > /usr/share/nginx/html/test.html
<!DOCTYPE html>
<html>
<head>
<title>Testing..</title>
</head>
<body>
<h1 style="color:rgb(90,70,250);">Hello, NodePort Service...!</h1>
<h2>Congratulations, you passed :-) </h2>
</body>
</html>
EOF
exit

Test using Pod IP:
~~~~~~~~~~~~~~~~~~~~~~~
kubectl get po -o wide
curl http://[POD-IP]/test.html
NodePort  –  Accessing using Service IP

Test using Service IP:
~~~~~~~~~~~~~~~~~~~~~~~~~~~
kubectl get svc -l app=nginx-app
curl http://[cluster-ip]/test.html

Test using Node IP (external IP)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
http://nodep-ip:nodePort/test.html
note: node-ip is the external ip address of a node.


*******************************************************************
4. Cleanup

kubectl delete -f nginx-deploy.yaml
kubectl delete -f nginx-svc.yaml
kubectl get deploy
kubectl get svc
kubectl get pods

*******************************************************************
 
 Load balancer pending and cluster Ip is pendig
 
 --------------------------------------------------------------------\
 
 VOLUMES--------------
 volume is just a directory with some data and is accesble to continer in pods.
 
 different kind of Volume :-
 
 ephemeral :-same life tym as pod
 durable :- beyond pod lyf tym
 
 
 Volume types
 1) Empty dir :- it will create an empty directory in Node where pod is schedule. container can read and write from this volume
 .once the pod died this volume also dies.
 2) Host volume -- it is like docker volume . mount a fileor directory form host node's file system into your path.
  dta remain after pod dies.
  
 another pod can created in same node and hostpath is present it will used the data.
 
 kubernets create hostpath volume in each worker node where pod is crated and volume is not in sync.
 
 
 -----------------------------
 
 epmtydir:
 
 apiVersion: v1
 kind: Pod
 metadata: 
   name: emptydirPod
 spec:
   containers:
   - images: k8s.gcr.io/test-webserver
     name: emptydirContianer
     volumeMounts:
     - name: cache-volume
       mountPath: /opt
     
   volumes:
   - name: cache-volume
     emptydir: {}
     
  
  
  =========================
  
  
  1) kubectl create -f emptydir.yaml
  2) kubectl get pods
  3) kubectl exec emptydirPod df /opt
  4) kubectl delete pod emptydirPod
  
  -----------------
  hostpath
  
  apiVersion: v1
  kind: Pod
  metadata:
    name: hostPathPod
  spec:
    containers:
    - name: redisConatiner
      image: redis
      volumeMounts:
      - mountpath: /opt
        name: hostpathVolume
        
      
    volumes:
    - name: hostpathVolume
      hostPath:
        path: /opt
	
# 2. Create and Display HostPath

kubectl create -f nginx-hostpath.yaml
kubectl get po
kubectl exec nginx-hostpath df /opt

*******************************************************************
3. Test: Creating "test" file underlying host dir & accessing from from pod

From HOST:
~~~~~~~~~~
cd /test-vol
echo "From Host" > from-host.txt
cat from-host.txt

From POD:
~~~~~~~~
kubectl exec nginx-hostpath cat /test-mnt/from-host.txt


*******************************************************************
4. Test: Creating "test" file inside the POD & accessing from underlying host dir

From POD:
~~~~~~~~~
kubectl exec nginx-hostpath -it -- /bin/sh
cd /test-mnt
echo "From Pod" > from-pod.txt
cat from-pod.txt

From Host:
~~~~~~~~~~
cd /test-vol
ls
cat from-pod.txt


*******************************************************************
5. Clean up

kubectl delete po nginx-hostpath
kubectl get po
ls /test-vol

      
 
========================================================

GCE persistnace disk

it is a persistance  volume in google cloud.

read write in one node and read only in other nodes


*******************************************************************
*
* Demo: gce Persistent Disk  |  Srinath Challa
*
*
*******************************************************************
1. Create Disk on Google Cloud

From Dashboard
(OR)
gcloud compute disks create --size=10GB --zone=us-central1-a my-data-disk
gcloud compute disks list

note: gcloud auth login

*******************************************************************
2. Pod with gcePersistentDisk YAML file

apiVersion: v1
kind: Pod
metadata:
  name: gce-pd
spec:
  containers:
  - name: nginx-container
    image: nginx
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    gcePersistentDisk:
      pdName: my-data-disk
      fsType: ext4

----------------------------------------------------

Bug Fix: For deploying Pod with gcePersistentDisk on GCE "VMs"
~~~~~~

a. Manually attach above created disk to worker node through GCE dashboard

b. Format above disk on the respective worker node where the above pod deployed
mkfs.ext4 /dev/disk/by-id/scsi-0Google_PersistentDisk_{Persistent Disk Name}

c Mount it in the location expected by Kubernetes. Run the following commands as root:
mkdir -p /var/lib/kubelet/plugins/kubernetes.io/gce-pd/mounts/{Persistent Disk Name} && mount /dev/disk/by-id/scsi-0Google_PersistentDisk_{Persistent Disk Name} /var/lib/kubelet/plugins/kubernetes.io/gce-pd/mounts/{Persistent Disk Name}

Reference: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/getting_started_with_kubernetes/get_started_provisioning_storage_in_kubernetes

----------------------------------------------------


*******************************************************************
3. Create and Display

kubectl create -f gcepd.yaml
kubectl get po –o wide
kubectl describe po gce-pd


*******************************************************************
4. Testing: Creating file inside mount where gcePersistentDisk volume is mounted and then delete

Create test file inside pod
---------------------------
kubectl exec gce-pd -it -- /bin/sh
df
echo "Testing - 1" > /test-pd/test1.html
exit

Delete pod:
-----------
kubectl delete -f gce-pd.yaml


*******************************************************************
5. Validate: Recreate Pod using same gcePersistentDisk above and validate above data

Recreate pod with same  config
------------------------------
kubectl create -f gce-pd.yaml
kubectl get po –o wide

Validate test file created in step#4 is still available?
--------------------------------------------------------
kubectl exec gce-pd -it /bin/sh
ls /test-pd/
cat /test-pd/test1.html


*******************************************************************
6. Cleanup

kubectl delete -f gce-pd.yaml
kubectl get pods


*******************************************************************




*******************************************************************
.
. Demo: Static Persistent Volumes  |  Srinath Challa
.

*******************************************************************
1. Create gcePersistent Disk on Google Cloud

From Dashboard
(OR)
gcloud compute disks create --size=10GB --zone=us-central1-a my-data-disk
gcloud compute disks list

note: create gcePersistentdisk in same zone as k8 cluster is in

*******************************************************************
2. Create "Persistent Volume(PV)" and display

YAML
~~~~~
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-gce
spec:
  capacity:
    storage: 15Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: slow
  gcePersistentDisk:
    pdName: my-data-disk
    fsType: ext4

Create:
~~~~~~
kubectl create -f pv.yaml
kubectl get pv
kubectl describe pv pv-gce 

*******************************************************************
3. Create "Persistent Volume Claim(PVC)" and display

YAML:
~~~~~
# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-disk-claim
spec:
  resources:
    requests:
      storage: 15Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: slow


Create & Display:
~~~~~~~~~~~~~~~~
kubectl create -f pvc.yaml
kubectl get pvc
kubectl get pv

*******************************************************************
4. Reference claim inside the Pod:

# nginx-pv.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
  - name: test-container
    image: nginx
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: pvc


Create & Display:
~~~~~~~~~~~~~~~~~
kubectl create -f nginx-pv.yaml
kubectl get po -o wide

*******************************************************************
5. Testing
	
a. Create a test file inside the mount where the disk is mounted

kubectl exec pv-pod -it -- /bin/sh
df /test-pd
cd /test-pd
echo "From first pod" > test1.txt
exit

-------------------------------------------------------------
b. Delete the Pod

kubectl delete -f nginx-pv.yaml

-------------------------------------------------------------
c. Recreate the Pod with same configuration

kubectl create -f nginx-pv.yaml
kubectl get pods -o wide

-------------------------------------------------------------
d. Verify the data created in step-1 is still available

kubectl exec pv-pod df /test-pd
kubectl exec pv-pod ls /test-pd/
kubectl exec pv-pod cat /test-pd/test1.html


*******************************************************************
# 6. Cleanup

kubectl delete -f nginx-pv.yaml
kubectl delete -f pvc.yaml
kubectl delete -f pv.yaml
kubectl get pods
kubectl get pvc
kubectl get pv

*******************************************************************





 

*******************************************************************
*
* Demo: Dynamic Volume Provisioning  |  Srinath Challa
*
*
*******************************************************************
1. Create Storage Classes

YAML:
~~~~~~~
# sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd

Create & Display:
~~~~~~~~~~~~~~~~~~
kubectl get sc
kubectl create -f sc.yaml
kubectl get sc
kubectl describe storageclass fast

*******************************************************************
2. Create Persistent Volume Claim (PVC)

# pvc1.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-disk-claim-1
spec:
  resources:
    requests:
      storage: 30Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  
-------------------------

# pvc2.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-disk-claim-2
spec:
  resources:
    requests:
      storage: 40Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  

Create & Display:
~~~~~~~~~~~~~~~~~
kubectl create -f pvc1.yaml
kubectl create -f pvc2.yaml
kubectl get pvc

*******************************************************************
3. Reference Claim inside Pod

# nginx-pv.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
  - name: test-container
    image: nginx
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: my-disk-claim-1

Create & Display:
~~~~~~~~~~~~~~~~
kubectl create -f nginx-pv.yaml
kubectl get po -o wide

*******************************************************************
4. Testing

a. Create a sample test file inside the mount.

kubectl exec pv-pod -it -- /bin/sh
df /test-pd
cd /test-pd 
echo "From test-1" > test1.txt            
exit

-------------------------------------------------------------
b. Delete the Pod

kubectl delete -f nginx-pv.yaml
kubectl get po –o wide

-------------------------------------------------------------
c. Recreate the Pod with same configuration

kubectl create -f nginx-pv.yaml
kubectl get po –o wide

-------------------------------------------------------------
d. Verify the data created in step-1 is still available?

kubectl exec pv-pod df /test-pd
kubectl exec pv-pod ls /test-pd/
kubectl exec pv-pod cat /test-pd/test1.txt

*******************************************************************
5.Cleanup
	
kubectl delete -f nginx-pv.yaml
kubectl delete -f pvc1.yaml
kubectl delete -f pvc2.yaml
kubectl delete -f sc.yaml
kubectl get pods
kubectl get pvc
kubectl get sc

******************************************************************* 
 
